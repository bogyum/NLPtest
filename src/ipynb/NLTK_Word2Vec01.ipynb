{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8e00824-0787-429a-92c4-6591d1a6e463",
   "metadata": {},
   "source": [
    "## Word2Vec 실습하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ce2b37-5ed1-47ff-b3a3-760dd83e2efb",
   "metadata": {},
   "source": [
    "### 1. English Word2Vec 만들기\n",
    "-------------------\n",
    "\n",
    "- gensim 라이브러리를 이용한 word2vec 만들기\n",
    "- 영어로 된 말뭉치(corpus)를 다운로드 받은 후 전처리하고, 상기 전처리된 데이터를 바탕으로 word2vec 을 만든다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "278d88a9-a179-4107-95b6-381c0dbc3fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 로딩\n",
    "import re\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from lxml import etree\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003cecfa-e5d9-41a2-9012-321b9516af38",
   "metadata": {},
   "source": [
    "##### 1) 훈련데이터 이해하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a64b0a5f-1220-43c2-9b33-2e47092059e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../../data/ted_en-20160408.xml', <http.client.HTTPMessage at 0x40ac9d4210>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 다운로드\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/09.%20Word%20Embedding/dataset/ted_en-20160408.xml\", filename=\"../../data/ted_en-20160408.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8b73d6-a3b8-438c-8464-0358296c48e9",
   "metadata": {},
   "source": [
    "- \\<content>\\</content> 사이에 위치한 데이터만 추출\n",
    "-  (Laughter), (Applause) 와 같은 배경음을 나타내는 단어 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db5f4e1-8994-4363-8ec8-f378c891418a",
   "metadata": {},
   "source": [
    "##### 2) 훈련 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e83cec0-fbc8-4034-bcc5-43bdacec3024",
   "metadata": {},
   "outputs": [],
   "source": [
    "targetXML = open('../../data/ted_en-20160408.xml', 'r', encoding='UTF8')\n",
    "targetText = etree.parse(targetXML)\n",
    "\n",
    "# xml 파일로부터 <content>와 </content> 사이의 내용만 가져온다.\n",
    "parseText = '\\n'.join(targetText.xpath('//content/text()'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f330b7ab-82fd-4ab3-adca-c01e6b2de27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규 표현식의 sub 모듈을 통해 content 중간에 등장하는 (Audio), (Laughter) 등의 배경음 부분을 제거.\n",
    "# 해당 코드는 괄호로 구성된 내용을 제거.\n",
    "contentText = re.sub(r'\\([^)]*\\)', '', parseText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5990938-e4c3-46dd-9638-f139d19f3a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here are two reasons companies fail: they only do more of the same, or they only do what's new.\\nTo me the real, real solution to quality growth is figuring out the balance between two activities: exploration and exploitation. Both are necessary, but it can be too much of a good thing.\\nConsider Facit. I'm actually old enough to remember them. Facit was a fantastic company. They were born deep in the Swedish forest, and they made the best mechanical calculators in the world. Everybody used them. A\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contentText[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06096093-4ccc-4d03-820f-3a0eb13988db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24062319\n"
     ]
    }
   ],
   "source": [
    "print(len(contentText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530fb4dc-8192-41ca-ab26-bf88ffe2154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 코퍼스에 대해서 NLTK를 이용하여 문장 토큰화를 수행.\n",
    "sentenceText = sent_tokenize(contentText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef19ea2e-7f6a-4bbb-b463-4a6b6b393c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환.\n",
    "from tqdm import tqdm\n",
    "\n",
    "normalizedText = []\n",
    "for string in tqdm(sentenceText):\n",
    "    tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
    "    normalizedText.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec80029-9fb4-4814-974a-4c6848f5a03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행\n",
    "result = [word_tokenize(sentence) for sentence in normalizedText]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c0080d-cf0d-4fa9-b08a-4bfa022788d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('총 샘플의 개수 : {}'.format(len(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b87c056-1eea-43d3-9bef-c5802e58d6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in result[:3]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f644bcf7-746e-4f12-9edc-5f966b95c754",
   "metadata": {},
   "source": [
    "##### 3) Word2Vec 학습(훈련) 시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b75ead-3f30-4129-b4f0-a0d6b5baba79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = Word2Vec(sentences=result, vector_size = 100, window=5, min_count=5, workers=4, sg=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7746bb1-11d2-4128-914a-eefa221f290a",
   "metadata": {},
   "source": [
    "* Word2Vec의 Hyper-parameter 값은 다음과 같습니다.\n",
    "\n",
    "    - vector_size = 워드 벡터의 특징 값. 즉, 임베딩된 벡터의 차원\n",
    "    - window = 컨텍스트 윈도우 크기\n",
    "    - min_count = 단어 최소 빈도수 제한(빈도가 적은 단어들은 제외)\n",
    "    - workers = 학습을 위한 프로세스 수\n",
    "    - sg = 0은 CBOW, 1은 Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c61a82b-6229-4ebc-a477-b433f3fea144",
   "metadata": {},
   "source": [
    "- model.wv.most_similar :: 단어 유사도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f937b2-0cce-4930-bbad-baebff2029c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result = model.wv.most_similar('man')\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b46535e-2cee-4815-91a8-ca119f89a21d",
   "metadata": {},
   "source": [
    "##### 4) Word2Vec 모델 저장하고 로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b293ea1-9dc2-4b08-b167-3d50ad18fad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format('eng_w2v') # 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8a4a83-3d70-47e0-809d-93ba07e7c18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = KeyedVectors.load_word2vec_format(\"eng_w2v\") # 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f937a56-2621-4195-a39f-734006545f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result = loaded_model.most_similar(\"man\")\n",
    "print(model_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
